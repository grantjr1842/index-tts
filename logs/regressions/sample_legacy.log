2025-11-26 02:18:33,560 WETEXT INFO found existing fst: /home/admin-grant-jr/github/index-tts/indextts/utils/tagger_cache/zh_tn_tagger.fst
2025-11-26 02:18:33,560 WETEXT INFO                     /home/admin-grant-jr/github/index-tts/indextts/utils/tagger_cache/zh_tn_verbalizer.fst
2025-11-26 02:18:33,560 WETEXT INFO skip building fst for zh_normalizer ...
2025-11-26 02:18:34,785 WETEXT INFO found existing fst: /home/admin-grant-jr/github/index-tts/.venv/lib/python3.10/site-packages/tn/en_tn_tagger.fst
2025-11-26 02:18:34,785 WETEXT INFO                     /home/admin-grant-jr/github/index-tts/.venv/lib/python3.10/site-packages/tn/en_tn_verbalizer.fst
2025-11-26 02:18:34,786 WETEXT INFO skip building fst for en_normalizer ...
>> GPT weights restored from: checkpoints/gpt.pth
>> semantic_codec weights restored from: ./checkpoints/hf_cache/models--amphion--MaskGCT/snapshots/265c6cef07625665d0c28d2faafb1415562379dc/semantic_codec/model.safetensors
cfm loaded
length_regulator loaded
gpt_layer loaded
>> s2mel weights restored from: checkpoints/s2mel.pth
>> campplus_model weights restored from: ./checkpoints/hf_cache/models--funasr--campplus/snapshots/fb71fe990cbf6031ae6987a2d76fe64f94377b7e/campplus_cn_common.bin
Loading weights from nvidia/bigvgan_v2_22khz_80band_256x
Removing weight norm...
>> bigvgan weights restored from: nvidia/bigvgan_v2_22khz_80band_256x
>> TextNormalizer loaded
>> bpe model loaded from: checkpoints/bpe.model
>> starting inference...
Use the specified emotion vector
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:00<00:09,  2.62it/s] 12%|█▏        | 3/25 [00:00<00:03,  5.96it/s] 16%|█▌        | 4/25 [00:00<00:03,  5.56it/s] 20%|██        | 5/25 [00:00<00:03,  5.33it/s] 24%|██▍       | 6/25 [00:01<00:03,  5.18it/s] 28%|██▊       | 7/25 [00:01<00:03,  5.09it/s] 32%|███▏      | 8/25 [00:01<00:03,  5.02it/s] 36%|███▌      | 9/25 [00:01<00:03,  4.98it/s] 40%|████      | 10/25 [00:01<00:03,  4.94it/s] 44%|████▍     | 11/25 [00:02<00:02,  4.92it/s] 48%|████▊     | 12/25 [00:02<00:02,  4.91it/s] 52%|█████▏    | 13/25 [00:02<00:02,  4.90it/s] 56%|█████▌    | 14/25 [00:02<00:02,  4.89it/s] 60%|██████    | 15/25 [00:03<00:02,  4.89it/s] 64%|██████▍   | 16/25 [00:03<00:01,  4.88it/s] 68%|██████▊   | 17/25 [00:03<00:01,  4.88it/s] 72%|███████▏  | 18/25 [00:03<00:01,  4.87it/s] 76%|███████▌  | 19/25 [00:03<00:01,  4.86it/s] 80%|████████  | 20/25 [00:04<00:01,  4.86it/s] 84%|████████▍ | 21/25 [00:04<00:00,  4.84it/s] 88%|████████▊ | 22/25 [00:04<00:00,  4.85it/s] 92%|█████████▏| 23/25 [00:04<00:00,  4.85it/s] 96%|█████████▌| 24/25 [00:04<00:00,  4.84it/s]100%|██████████| 25/25 [00:05<00:00,  4.84it/s]100%|██████████| 25/25 [00:05<00:00,  4.92it/s]
torch.Size([1, 92416])
>> gpt_gen_time: 20.07 seconds
>> gpt_forward_time: 0.05 seconds
>> s2mel_time: 5.11 seconds
>> bigvgan_time: 4.27 seconds
>> Total inference time: 33.41 seconds
>> Generated audio length: 4.19 seconds
>> RTF: 7.9705
>> starting inference...
Use the specified emotion vector
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:00<00:03,  7.95it/s] 12%|█▏        | 3/25 [00:00<00:02, 10.43it/s] 20%|██        | 5/25 [00:00<00:03,  6.55it/s] 24%|██▍       | 6/25 [00:00<00:03,  5.98it/s] 28%|██▊       | 7/25 [00:01<00:03,  5.62it/s] 32%|███▏      | 8/25 [00:01<00:03,  5.37it/s] 36%|███▌      | 9/25 [00:01<00:03,  5.20it/s] 40%|████      | 10/25 [00:01<00:02,  5.08it/s] 44%|████▍     | 11/25 [00:01<00:02,  5.00it/s] 48%|████▊     | 12/25 [00:02<00:02,  4.94it/s] 52%|█████▏    | 13/25 [00:02<00:02,  4.91it/s] 56%|█████▌    | 14/25 [00:02<00:02,  4.86it/s] 60%|██████    | 15/25 [00:02<00:02,  4.86it/s] 64%|██████▍   | 16/25 [00:02<00:01,  4.84it/s] 68%|██████▊   | 17/25 [00:03<00:01,  4.83it/s] 72%|███████▏  | 18/25 [00:03<00:01,  4.79it/s] 76%|███████▌  | 19/25 [00:03<00:01,  4.82it/s] 96%|█████████▌| 24/25 [00:03<00:00, 10.31it/s]100%|██████████| 25/25 [00:04<00:00,  8.68it/s]100%|██████████| 25/25 [00:04<00:00,  6.17it/s]
torch.Size([1, 98048])
>> gpt_gen_time: 13.19 seconds
>> gpt_forward_time: 0.04 seconds
>> s2mel_time: 4.88 seconds
>> bigvgan_time: 3.30 seconds
>> Total inference time: 21.63 seconds
>> Generated audio length: 4.45 seconds
>> RTF: 4.8653
[1] wrote data_stereo/demo_001.wav (duration=4.45s, cumulative=4.45s)
>> starting inference...
Use the specified emotion vector
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:00<00:07,  3.12it/s] 16%|█▌        | 4/25 [00:00<00:02,  9.90it/s] 24%|██▍       | 6/25 [00:00<00:01, 10.79it/s] 32%|███▏      | 8/25 [00:00<00:01, 11.28it/s] 40%|████      | 10/25 [00:00<00:01, 11.52it/s] 48%|████▊     | 12/25 [00:01<00:01, 11.71it/s] 56%|█████▌    | 14/25 [00:01<00:00, 11.84it/s] 64%|██████▍   | 16/25 [00:01<00:00, 11.90it/s] 72%|███████▏  | 18/25 [00:01<00:00, 11.94it/s] 80%|████████  | 20/25 [00:01<00:00, 11.99it/s] 88%|████████▊ | 22/25 [00:01<00:00, 11.83it/s] 96%|█████████▌| 24/25 [00:02<00:00, 12.10it/s]100%|██████████| 25/25 [00:02<00:00, 11.33it/s]
torch.Size([1, 76544])
>> gpt_gen_time: 8.08 seconds
>> gpt_forward_time: 0.04 seconds
>> s2mel_time: 2.22 seconds
>> bigvgan_time: 2.76 seconds
>> Total inference time: 13.16 seconds
>> Generated audio length: 3.47 seconds
>> RTF: 3.7917
>> starting inference...
Use the specified emotion vector
  0%|          | 0/25 [00:00<?, ?it/s]  8%|▊         | 2/25 [00:00<00:01, 15.24it/s] 16%|█▌        | 4/25 [00:00<00:03,  6.51it/s] 20%|██        | 5/25 [00:00<00:03,  5.81it/s] 24%|██▍       | 6/25 [00:00<00:03,  5.40it/s] 28%|██▊       | 7/25 [00:01<00:03,  5.15it/s] 48%|████▊     | 12/25 [00:01<00:01, 10.16it/s] 56%|█████▌    | 14/25 [00:01<00:01,  7.56it/s] 60%|██████    | 15/25 [00:02<00:01,  6.83it/s] 64%|██████▍   | 16/25 [00:02<00:01,  6.24it/s] 68%|██████▊   | 17/25 [00:02<00:01,  5.80it/s] 72%|███████▏  | 18/25 [00:02<00:01,  5.45it/s] 76%|███████▌  | 19/25 [00:03<00:01,  5.21it/s] 80%|████████  | 20/25 [00:03<00:00,  5.02it/s] 84%|████████▍ | 21/25 [00:03<00:00,  4.90it/s] 88%|████████▊ | 22/25 [00:03<00:00,  4.80it/s] 92%|█████████▏| 23/25 [00:03<00:00,  4.74it/s] 96%|█████████▌| 24/25 [00:04<00:00,  4.69it/s]100%|██████████| 25/25 [00:04<00:00,  4.66it/s]100%|██████████| 25/25 [00:04<00:00,  5.80it/s]
torch.Size([1, 109056])
>> gpt_gen_time: 13.12 seconds
>> gpt_forward_time: 0.04 seconds
>> s2mel_time: 5.15 seconds
>> bigvgan_time: 3.52 seconds
>> Total inference time: 22.13 seconds
>> Generated audio length: 4.95 seconds
>> RTF: 4.4749
[2] wrote data_stereo/demo_002.wav (duration=4.95s, cumulative=9.39s)

Done. JSONL index written to: /tmp/moshi_regress_sample_legacy/moshi_sample_legacy.jsonl
Summary -> samples: 2, total duration: 9.39s, avg: 4.70s
You can now run (from moshi-finetune repo):
  python annotate.py /tmp/moshi_regress_sample_legacy/moshi_sample_legacy.jsonl
