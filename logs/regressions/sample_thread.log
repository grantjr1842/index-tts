2025-11-26 02:20:59,737 WETEXT INFO found existing fst: /home/admin-grant-jr/github/index-tts/indextts/utils/tagger_cache/zh_tn_tagger.fst
2025-11-26 02:20:59,737 WETEXT INFO                     /home/admin-grant-jr/github/index-tts/indextts/utils/tagger_cache/zh_tn_verbalizer.fst
2025-11-26 02:20:59,737 WETEXT INFO skip building fst for zh_normalizer ...
2025-11-26 02:21:00,280 WETEXT INFO found existing fst: /home/admin-grant-jr/github/index-tts/.venv/lib/python3.10/site-packages/tn/en_tn_tagger.fst
2025-11-26 02:21:00,280 WETEXT INFO                     /home/admin-grant-jr/github/index-tts/.venv/lib/python3.10/site-packages/tn/en_tn_verbalizer.fst
2025-11-26 02:21:00,280 WETEXT INFO skip building fst for en_normalizer ...
Using thread backend with worker-count=2 on device=cuda:0
>> GPT weights restored from: checkpoints/gpt.pth
>> semantic_codec weights restored from: ./checkpoints/hf_cache/models--amphion--MaskGCT/snapshots/265c6cef07625665d0c28d2faafb1415562379dc/semantic_codec/model.safetensors
cfm loaded
length_regulator loaded
gpt_layer loaded
>> s2mel weights restored from: checkpoints/s2mel.pth
>> campplus_model weights restored from: ./checkpoints/hf_cache/models--funasr--campplus/snapshots/fb71fe990cbf6031ae6987a2d76fe64f94377b7e/campplus_cn_common.bin
Loading weights from nvidia/bigvgan_v2_22khz_80band_256x
Removing weight norm...
>> bigvgan weights restored from: nvidia/bigvgan_v2_22khz_80band_256x
>> TextNormalizer loaded
>> bpe model loaded from: checkpoints/bpe.model
>> starting inference...
Use the specified emotion vector
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:00<00:07,  3.26it/s] 12%|█▏        | 3/25 [00:00<00:03,  6.51it/s] 16%|█▌        | 4/25 [00:00<00:03,  5.84it/s] 20%|██        | 5/25 [00:00<00:03,  5.49it/s] 24%|██▍       | 6/25 [00:01<00:03,  5.28it/s] 28%|██▊       | 7/25 [00:01<00:03,  5.14it/s] 32%|███▏      | 8/25 [00:01<00:03,  5.05it/s] 36%|███▌      | 9/25 [00:01<00:03,  4.99it/s] 40%|████      | 10/25 [00:01<00:03,  4.95it/s] 44%|████▍     | 11/25 [00:02<00:02,  4.92it/s] 48%|████▊     | 12/25 [00:02<00:02,  4.90it/s] 52%|█████▏    | 13/25 [00:02<00:02,  4.88it/s] 56%|█████▌    | 14/25 [00:02<00:02,  4.86it/s] 60%|██████    | 15/25 [00:02<00:02,  4.85it/s] 64%|██████▍   | 16/25 [00:03<00:01,  4.84it/s] 68%|██████▊   | 17/25 [00:03<00:01,  4.84it/s] 72%|███████▏  | 18/25 [00:03<00:01,  4.84it/s] 76%|███████▌  | 19/25 [00:03<00:01,  4.84it/s] 80%|████████  | 20/25 [00:04<00:01,  4.83it/s] 84%|████████▍ | 21/25 [00:04<00:00,  4.84it/s] 88%|████████▊ | 22/25 [00:04<00:00,  4.83it/s] 92%|█████████▏| 23/25 [00:04<00:00,  4.83it/s] 96%|█████████▌| 24/25 [00:04<00:00,  4.83it/s]100%|██████████| 25/25 [00:05<00:00,  4.83it/s]100%|██████████| 25/25 [00:05<00:00,  4.95it/s]
torch.Size([1, 90624])
>> gpt_gen_time: 10.82 seconds
>> gpt_forward_time: 0.04 seconds
>> s2mel_time: 5.08 seconds
>> bigvgan_time: 3.55 seconds
>> Total inference time: 23.20 seconds
>> Generated audio length: 4.11 seconds
>> RTF: 5.6455
>> starting inference...
Use the specified emotion vector
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:00<00:03,  6.84it/s] 12%|█▏        | 3/25 [00:00<00:02,  8.83it/s] 16%|█▌        | 4/25 [00:00<00:03,  6.84it/s] 20%|██        | 5/25 [00:00<00:03,  5.96it/s] 24%|██▍       | 6/25 [00:00<00:03,  5.53it/s] 28%|██▊       | 7/25 [00:01<00:03,  4.70it/s] 32%|███▏      | 8/25 [00:01<00:03,  4.90it/s] 36%|███▌      | 9/25 [00:01<00:03,  4.83it/s] 40%|████      | 10/25 [00:01<00:03,  4.76it/s] 44%|████▍     | 11/25 [00:02<00:02,  4.73it/s] 48%|████▊     | 12/25 [00:02<00:02,  4.72it/s] 52%|█████▏    | 13/25 [00:02<00:02,  4.54it/s] 56%|█████▌    | 14/25 [00:02<00:02,  4.63it/s] 60%|██████    | 15/25 [00:02<00:02,  4.75it/s] 64%|██████▍   | 16/25 [00:03<00:01,  4.90it/s] 68%|██████▊   | 17/25 [00:03<00:01,  4.83it/s] 72%|███████▏  | 18/25 [00:03<00:01,  4.84it/s] 76%|███████▌  | 19/25 [00:03<00:01,  4.68it/s] 80%|████████  | 20/25 [00:03<00:01,  4.81it/s] 84%|████████▍ | 21/25 [00:04<00:00,  4.75it/s] 88%|████████▊ | 22/25 [00:04<00:00,  4.76it/s] 92%|█████████▏| 23/25 [00:04<00:00,  4.59it/s] 96%|█████████▌| 24/25 [00:04<00:00,  4.59it/s]100%|██████████| 25/25 [00:05<00:00,  4.70it/s]100%|██████████| 25/25 [00:05<00:00,  4.94it/s]
torch.Size([1, 103424])
>> gpt_gen_time: 11.41 seconds
>> gpt_forward_time: 0.05 seconds
>> s2mel_time: 5.08 seconds
>> bigvgan_time: 3.66 seconds
>> Total inference time: 20.35 seconds
>> Generated audio length: 4.69 seconds
>> RTF: 4.3393
>> starting inference...
[1] wrote data_stereo/demo_001.wav (duration=4.69s, cumulative=4.69s)
Use the specified emotion vector
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:00<00:07,  3.18it/s] 12%|█▏        | 3/25 [00:00<00:02,  8.09it/s] 20%|██        | 5/25 [00:00<00:02,  9.50it/s] 28%|██▊       | 7/25 [00:00<00:01, 10.27it/s] 36%|███▌      | 9/25 [00:00<00:01, 10.71it/s] 44%|████▍     | 11/25 [00:01<00:01, 10.99it/s] 52%|█████▏    | 13/25 [00:01<00:01, 11.15it/s] 60%|██████    | 15/25 [00:01<00:00, 11.21it/s] 68%|██████▊   | 17/25 [00:01<00:00, 11.29it/s] 76%|███████▌  | 19/25 [00:01<00:00, 11.36it/s] 84%|████████▍ | 21/25 [00:01<00:00, 11.39it/s] 92%|█████████▏| 23/25 [00:02<00:00, 11.41it/s]100%|██████████| 25/25 [00:02<00:00, 11.43it/s]100%|██████████| 25/25 [00:02<00:00, 10.66it/s]
torch.Size([1, 82688])
>> gpt_gen_time: 11.84 seconds
>> gpt_forward_time: 0.05 seconds
>> s2mel_time: 2.36 seconds
>> bigvgan_time: 2.97 seconds
>> Total inference time: 17.49 seconds
>> Generated audio length: 3.75 seconds
>> RTF: 4.6641
>> starting inference...
Use the specified emotion vector
  0%|          | 0/25 [00:00<?, ?it/s]  8%|▊         | 2/25 [00:00<00:01, 14.40it/s] 16%|█▌        | 4/25 [00:00<00:03,  6.46it/s] 20%|██        | 5/25 [00:00<00:03,  5.82it/s] 24%|██▍       | 6/25 [00:00<00:03,  5.45it/s] 28%|██▊       | 7/25 [00:01<00:03,  5.20it/s] 32%|███▏      | 8/25 [00:01<00:03,  5.05it/s] 36%|███▌      | 9/25 [00:01<00:03,  4.94it/s] 40%|████      | 10/25 [00:01<00:03,  4.87it/s] 44%|████▍     | 11/25 [00:02<00:02,  4.81it/s] 48%|████▊     | 12/25 [00:02<00:02,  4.78it/s] 52%|█████▏    | 13/25 [00:02<00:02,  4.75it/s] 56%|█████▌    | 14/25 [00:02<00:02,  4.73it/s] 60%|██████    | 15/25 [00:02<00:02,  4.72it/s] 64%|██████▍   | 16/25 [00:03<00:01,  4.71it/s] 68%|██████▊   | 17/25 [00:03<00:01,  4.70it/s] 72%|███████▏  | 18/25 [00:03<00:01,  4.69it/s] 76%|███████▌  | 19/25 [00:03<00:01,  4.69it/s] 80%|████████  | 20/25 [00:03<00:01,  4.69it/s] 84%|████████▍ | 21/25 [00:04<00:00,  4.68it/s] 88%|████████▊ | 22/25 [00:04<00:00,  4.68it/s] 92%|█████████▏| 23/25 [00:04<00:00,  4.68it/s] 96%|█████████▌| 24/25 [00:04<00:00,  4.67it/s]100%|██████████| 25/25 [00:05<00:00,  4.64it/s]100%|██████████| 25/25 [00:05<00:00,  4.95it/s]
torch.Size([1, 98560])
>> gpt_gen_time: 11.25 seconds
>> gpt_forward_time: 0.05 seconds
>> s2mel_time: 5.06 seconds
>> bigvgan_time: 3.46 seconds
>> Total inference time: 19.99 seconds
>> Generated audio length: 4.47 seconds
>> RTF: 4.4713
[2] wrote data_stereo/demo_002.wav (duration=4.47s, cumulative=9.16s)
[threads] Parallel pipeline complete in 81.04s — samples: 2, total duration: 9.16s

Done. JSONL index written to: /tmp/moshi_regress_sample_threaded/moshi_sample_threaded.jsonl
Summary -> samples: 2, total duration: 9.16s, avg: 4.58s
You can now run (from moshi-finetune repo):
  python annotate.py /tmp/moshi_regress_sample_threaded/moshi_sample_threaded.jsonl
